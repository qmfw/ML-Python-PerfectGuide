{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP(Natural Language Processing)\n",
    "\n",
    "인간의 언어를 이해하고 해석\n",
    "\n",
    "### 텍스트 분석\n",
    "\n",
    "머신러닝, 언어이해, 통계 활용해 모델 수립하고 정보 추출해 비즈니스 인텔리전스나 예측분석 등 분석 작업 주로 수행\n",
    "\n",
    "**텍스트 분류(Text Classification)**\n",
    "문서가 특정분류 또는 카테고리에 속하는 것을 예측하는 기법    \n",
    "ex) 특정 신문기사 내용이 연애/정치/사회/문화 중 어떤 카테고리에 속하는지 자동 분류 혹은 스팸메일 검출, 지도학습 적용    \n",
    "**감성 분석(Sentiment Analysis)**    \n",
    "텍스트에서 나타나는 감정/판단/믿음/의견/기분 등 주관적 요소 분석 기법    \n",
    "소셜 미디어 감정분석, 영화나 제품에 대한 긍정또는 리뷰, 여론조사 의견분석 등 활용    \n",
    "지도학습/비지도학습 이용해 적용    \n",
    "**텍스트 요약(Summarization)**    \n",
    "텍스트 내 중요한 주제나 중심 사상 추출하는 기법, 대표적으로 토픽 모델링    \n",
    "**텍스트 군집화와 유사도 측정**    \n",
    "비슷한 유형의 문서에 대해 군집화 수행하는 기법    \n",
    "텍스트 분류를 비지도학습으로 수행하는 방법으로 사용    \n",
    "문서들간의 유사도 측정해 비슷한 문서끼리 모을 수 있는 방법    \n",
    "\n",
    "1. Text 문서를 데이터사전 가공 후 Feature Vectorization 수행    \n",
    "2. Bag of Words 또는 Word2Vec에 Feature 기반의 데이터셋 제공    \n",
    "3. ML 학습/예측/평가    \n",
    "\n",
    "### 텍스트 전처리 - 텍스트 정규화\n",
    "\n",
    "**클렌징(Cleansing)**    \n",
    "텍스트에 분석에 방해되는 불필요한 문자, 기호 등 사전에 제거 ex) HTML, XML 태그나 특정 기호    \n",
    "**토큰화(Tokenization)**    \n",
    "문장 토큰화, 단어 토큰화, n-gram    \n",
    "* N-gram    \n",
    "문장 개별단어 하나씩 토큰화할 경우 문맥적 의미 무시, 문제 해결 위해 도입    \n",
    "\"Agent Smith knocks the door\"를 2-gram(bigram)으로 만들면 (Agent, Smith), (Smith, knocks), (knocks, the), (the, door)    \n",
    "\n",
    "**필터링/스톱워드 제거/철자 수정**    \n",
    "불필요한 단어나 분석에 큰 의미없는 단어(a, the, is, will등)와 잘못된 철자 수정    \n",
    "**Stemming/Lemmatization**    \n",
    "어근(단어 원형) 추출, Lemmatization이 Stemming보다 정교하고 의미론적 기반에서 단어 원형을 찾아줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**문장 토큰화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "#nltk 직접 쓰는 경우는 없음\n",
    "from nltk import sent_tokenize\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room.  \\\n",
    "              You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(type(sentences),len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단어 토큰화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**여러 문장들에 대한 단어 토큰화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "#여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "#여러 문장들에 대해 문장별 단어 토큰화 수행. \n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens),len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n-gram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'Matrix'), ('Matrix', 'is'), ('is', 'everywhere'), ('everywhere', 'its'), ('its', 'all'), ('all', 'around'), ('around', 'us'), ('us', ','), (',', 'here'), ('here', 'even'), ('even', 'in'), ('in', 'this'), ('this', 'room'), ('room', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "all_ngrams = ngrams(words, 2)\n",
    "ngrams = [ngram for ngram in all_ngrams]\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KwonChulmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 갯수: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this']\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop words 갯수:',len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "# 위 예제의 3개의 문장별로 얻은 word_tokens list 에 대해 stop word 제거 Loop\n",
    "for sentence in word_tokens:\n",
    "    filtered_words=[]\n",
    "    # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거 Loop\n",
    "    for word in sentence:\n",
    "        #소문자로 모두 변환합니다. \n",
    "        word = word.lower()\n",
    "        # tokenize 된 개별 word가 stop words 들의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming과 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Bag of Words – BOW\n",
    "\n",
    "**BOW**    \n",
    "모든 단어들을 문맥 순서 무시하고 일괄적으로 단어에 대해 빈도값 부여해 피처 추출    \n",
    "문서내 모든 단어를 한꺼번에 봉투 안에 넣은 뒤 흔들어 섞는다는 의미    \n",
    "Document Term Matrix: 개별 문서(or 문장)을 단어들의 횟수나 정규화 변환된 횟수로 표현\n",
    "\n",
    "**Word Embedding (Word2Vec)**    \n",
    "개별 단어를 문맥을 가지는 N차원 공간에 벡터로 표현    \n",
    "\n",
    "### Bag-of-Words 구조\n",
    "\n",
    "1. 문장 1과 2에 있는 모든 단어에서 중복 제거 후 각 단어를 컬럼 형태로 나열 후 각 단어에 고유 인덱스 부여    \n",
    "2. 문장에서 해당단어 나타나는 횟수를 각 단어에 기재\n",
    "\n",
    "**장점**    \n",
    "* 쉽고 빠른 구축    \n",
    "* 예상보다 문서의 특징을 잘 나타내어 여러분야에서 활용도 높음    \n",
    "\n",
    "**단점**    \n",
    "* 문맥 의미(Semantic Context) 반영 문제    \n",
    "* 희소 행렬 문제 (0으로 빈 값 많아져 메모리 낭비)    \n",
    "\n",
    "### BOW 피처 벡터화 유형    \n",
    "\n",
    "**단순 카운트 기반의 벡터화**    \n",
    "단어 피처에 값을 부여할 때 각 문서에서 해당단어 나타나는 횟수, count 부여 횟수    \n",
    "카운트 벡터화에서 카운트 값 높을수록 중요 단어로 인식    \n",
    "\n",
    "**TF-IDF 벡터화**    \n",
    "언어 특성상 문장에서 자주 사용될 수밖에 없는 단어까지 높은 값 부여하는 문제해결    \n",
    "모든 문서에서 전반적으로 자주 나타나는 단어에 페널티 주는 방식으로 값 부여    \n",
    "\n",
    "**TF-IDF(Term Frequency Inverse Document Frequency)**    \n",
    "특정 단어가 다른문서에 나타나지 않고 특정 문서에서만 자주 사용시 해당 단어는 중요 단어일 가능성 높음    \n",
    "특정 단어가 매우 많은 여러 문서에서 빈번히 나타나면 해당 단어는 개별문서 특징짓는 정보로서 의미 상실\n",
    "\n",
    "* TF(Term Frequency)    \n",
    "문서에서 해당단어가 얼마나 나왔는지 나타내는 지표\n",
    "\n",
    "* DF(Document Frequency)    \n",
    "해당 단어가 몇개의 문서에서 나타났는지 나타내는 지표\n",
    "\n",
    "* IDF(Inverse Document Frequency)    \n",
    "DF의 역수로서 전체문서수 / DF    \n",
    "\n",
    "TFIDFi = TFi * log (N / DFi)    \n",
    "TFi = 개별 문서에서의 단어 i 빈도    \n",
    "DFi = 단어 i를 가지고 있는 문서 개수    \n",
    "N = 전체 문서 개수\n",
    "\n",
    "### 사이킷런 CountVectorizer\n",
    "\n",
    "**max_df**    \n",
    "너무 높은 빈도수 가지는 단어 피처 제외 위한 파라미터    \n",
    "max_df=100(정수) then 100개 이하로 나타나는 단어만 피처로 추출    \n",
    "max_df=0.95(실수) then 빈도수 0~95%까지의 단어만 피처로 추출하고 나머지 상위 5%는 추출하지 않음    \n",
    "**min_df**    \n",
    "너무 낮은 빈도수 가지는 단어 피처 제외 위한 파라미터    \n",
    "min_df=2(정수) then 2번 이하로 나타나는 단어는 피처로 추출하지 않음    \n",
    "min_df=0.02(실수) then 하위 2% 이하의 빈도수 가지는 단어는 피처로 추출하지 않음    \n",
    "**max_features**    \n",
    "피처의 개수 제한하며 정수로 값 지정    \n",
    "max_features=2000 then 가장 높은 빈도 가지는 단어 순으로 정렬해 2000개까지 피처로 추출    \n",
    "**stop_words**    \n",
    "'english'로 지정하면 영어의 스톱워드로 지정된 단어는 추출에서 제외    \n",
    "**ngram_range**    \n",
    "n-gram 범위 설정, 튜플 형태로 (범위 최솟값, 범위 최댓값), 디폴트 (1,1)로 지정시 토근화된 단어 1개씩 피처로 추출    \n",
    "(1,2)로 지정시 토큰화된 단어를 1개씩(min 1), 그리고 순서대로 2개씩(max 2) 묶어서 피처로 추출    \n",
    "**analyzer**    \n",
    "피처 추출 수행한 단위 지정, 디폴트 'word'    \n",
    "Word가 아니라 character의 특정범위 피처로 만드는 특정한 경우 등 사용    \n",
    "**token_pattern**    \n",
    "정규 표현식 패턴 지정, 디폴트는 '\\b\\w\\w+\\b', \\b 사이의 2문자 이상의 단어를 토큰으로 분리    \n",
    "analyzer='word'로 설정시만 변경 가능하나 디폴트 변경 경우 거의 발생하지 않음    \n",
    "**lower_case**    \n",
    "모든 문자를 소문자로 변경할 것인지 설정, 디폴트 True    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**사이킷런 CountVectorizer 테스트**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everywhere its all around us, here even in this room.                   You can see it out your window or on your television.                   You feel it when you go to work, or go to church or pay your taxes.', 'You take the blue pill and the story ends.  You wake in your bed and you believe whatever you want to believe                  You take the red pill and you stay in Wonderland and I show you how deep the rabbit-hole goes.'] \n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "text_sample_01 = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "                  You can see it out your window or on your television. \\\n",
    "                  You feel it when you go to work, or go to church or pay your taxes.'\n",
    "text_sample_02 = 'You take the blue pill and the story ends.  You wake in your bed and you believe whatever you want to believe\\\n",
    "                  You take the red pill and you stay in Wonderland and I show you how deep the rabbit-hole goes.'\n",
    "text=[]\n",
    "text.append(text_sample_01); text.append(text_sample_02)\n",
    "print(text,\"\\n\", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CountVectorizer객체 생성 후 fit(), transform()으로 텍스트에 대한 feature vectorization 수행**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Count Vectorization으로 feature extraction 변환 수행. \n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr_vect = cnt_vect.transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**피처 벡터화 후 데이터 유형 및 여러 속성 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 51)\n",
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t2\n",
      "  (0, 15)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t2\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 24)\t3\n",
      "  (0, 25)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 37)\t1\n",
      "  (0, 38)\t1\n",
      "  (0, 39)\t1\n",
      "  (0, 40)\t2\n",
      "  :\t:\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 18)\t2\n",
      "  (1, 27)\t2\n",
      "  (1, 28)\t1\n",
      "  (1, 29)\t1\n",
      "  (1, 32)\t1\n",
      "  (1, 33)\t1\n",
      "  (1, 34)\t1\n",
      "  (1, 35)\t2\n",
      "  (1, 38)\t4\n",
      "  (1, 40)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 43)\t1\n",
      "  (1, 44)\t1\n",
      "  (1, 47)\t1\n",
      "  (1, 49)\t7\n",
      "  (1, 50)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(ftr_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 38, 'matrix': 22, 'is': 19, 'everywhere': 11, 'its': 21, 'all': 0, 'around': 2, 'us': 41, 'here': 15, 'even': 10, 'in': 18, 'this': 39, 'room': 30, 'you': 49, 'can': 6, 'see': 31, 'it': 20, 'out': 25, 'your': 50, 'window': 46, 'or': 24, 'on': 23, 'television': 37, 'feel': 12, 'when': 45, 'go': 13, 'to': 40, 'work': 48, 'church': 7, 'pay': 26, 'taxes': 36, 'take': 35, 'blue': 5, 'pill': 27, 'and': 1, 'story': 34, 'ends': 9, 'wake': 42, 'bed': 3, 'believe': 4, 'whatever': 44, 'want': 43, 'red': 29, 'stay': 33, 'wonderland': 47, 'show': 32, 'how': 17, 'deep': 8, 'rabbit': 28, 'hole': 16, 'goes': 14}\n"
     ]
    }
   ],
   "source": [
    "print(cnt_vect.vocabulary_) #index를 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vect = CountVectorizer(max_features=5)\n",
    "cnt_vect.fit(text)\n",
    "ftr_vect = cnt_vect.transform(text)\n",
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(cnt_vect.vocabulary_)\n",
    "# {'the': 2, 'you': 3, 'your': 4, 'or': 1, 'and': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 5)\n",
      "{'window': 4, 'pill': 1, 'wake': 2, 'believe': 0, 'want': 3}\n"
     ]
    }
   ],
   "source": [
    "cnt_vect = CountVectorizer(max_features=5, stop_words='english')\n",
    "cnt_vect.fit(text)\n",
    "ftr_vect = cnt_vect.transform(text)\n",
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(cnt_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ngram_range 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 201)\n",
      "{'the': 129, 'matrix': 77, 'is': 66, 'everywhere': 40, 'its': 74, 'all': 0, 'around': 11, 'us': 150, 'here': 51, 'even': 37, 'in': 59, 'this': 140, 'room': 106, 'you': 174, 'can': 25, 'see': 109, 'it': 69, 'out': 90, 'your': 193, 'window': 165, 'or': 83, 'on': 80, 'television': 126, 'feel': 43, 'when': 162, 'go': 46, 'to': 143, 'work': 171, 'church': 28, 'pay': 93, 'taxes': 125, 'the matrix': 132, 'matrix is': 78, 'is everywhere': 67, 'everywhere its': 41, 'its all': 75, 'all around': 1, 'around us': 12, 'us here': 151, 'here even': 52, 'even in': 38, 'in this': 60, 'this room': 141, 'room you': 107, 'you can': 177, 'can see': 26, 'see it': 110, 'it out': 70, 'out your': 91, 'your window': 199, 'window or': 166, 'or on': 86, 'on your': 81, 'your television': 197, 'television you': 127, 'you feel': 179, 'feel it': 44, 'it when': 72, 'when you': 163, 'you go': 181, 'go to': 47, 'to work': 148, 'work or': 172, 'or go': 84, 'to church': 146, 'church or': 29, 'or pay': 88, 'pay your': 94, 'your taxes': 196, 'the matrix is': 133, 'matrix is everywhere': 79, 'is everywhere its': 68, 'everywhere its all': 42, 'its all around': 76, 'all around us': 2, 'around us here': 13, 'us here even': 152, 'here even in': 53, 'even in this': 39, 'in this room': 61, 'this room you': 142, 'room you can': 108, 'you can see': 178, 'can see it': 27, 'see it out': 111, 'it out your': 71, 'out your window': 92, 'your window or': 200, 'window or on': 167, 'or on your': 87, 'on your television': 82, 'your television you': 198, 'television you feel': 128, 'you feel it': 180, 'feel it when': 45, 'it when you': 73, 'when you go': 164, 'you go to': 182, 'go to work': 49, 'to work or': 149, 'work or go': 173, 'or go to': 85, 'go to church': 48, 'to church or': 147, 'church or pay': 30, 'or pay your': 89, 'pay your taxes': 95, 'take': 121, 'blue': 22, 'pill': 96, 'and': 3, 'story': 118, 'ends': 34, 'wake': 153, 'bed': 14, 'believe': 17, 'whatever': 159, 'want': 156, 'red': 103, 'stay': 115, 'wonderland': 168, 'show': 112, 'how': 56, 'deep': 31, 'rabbit': 100, 'hole': 54, 'goes': 50, 'you take': 187, 'take the': 122, 'the blue': 130, 'blue pill': 23, 'pill and': 97, 'and the': 6, 'the story': 138, 'story ends': 119, 'ends you': 35, 'you wake': 189, 'wake in': 154, 'in your': 64, 'your bed': 194, 'bed and': 15, 'and you': 8, 'you believe': 175, 'believe whatever': 18, 'whatever you': 160, 'you want': 191, 'want to': 157, 'to believe': 144, 'believe you': 20, 'the red': 136, 'red pill': 104, 'you stay': 185, 'stay in': 116, 'in wonderland': 62, 'wonderland and': 169, 'and show': 4, 'show you': 113, 'you how': 183, 'how deep': 57, 'deep the': 32, 'the rabbit': 134, 'rabbit hole': 101, 'hole goes': 55, 'you take the': 188, 'take the blue': 123, 'the blue pill': 131, 'blue pill and': 24, 'pill and the': 98, 'and the story': 7, 'the story ends': 139, 'story ends you': 120, 'ends you wake': 36, 'you wake in': 190, 'wake in your': 155, 'in your bed': 65, 'your bed and': 195, 'bed and you': 16, 'and you believe': 9, 'you believe whatever': 176, 'believe whatever you': 19, 'whatever you want': 161, 'you want to': 192, 'want to believe': 158, 'to believe you': 145, 'believe you take': 21, 'take the red': 124, 'the red pill': 137, 'red pill and': 105, 'pill and you': 99, 'and you stay': 10, 'you stay in': 186, 'stay in wonderland': 117, 'in wonderland and': 63, 'wonderland and show': 170, 'and show you': 5, 'show you how': 114, 'you how deep': 184, 'how deep the': 58, 'deep the rabbit': 33, 'the rabbit hole': 135, 'rabbit hole goes': 102}\n"
     ]
    }
   ],
   "source": [
    "cnt_vect = CountVectorizer(ngram_range=(1,3))\n",
    "cnt_vect.fit(text)\n",
    "ftr_vect = cnt_vect.transform(text)\n",
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(cnt_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 희소 행렬 - COO 형식\n",
    "\n",
    "**COO 형식**    \n",
    "Coordinate 방식 의미하며 0 아닌 데이터만 별도 배열 저장하고 그 데이터 가리키는 행과 열 위치를 별도 배열로 저장    \n",
    "**CSR 형식**    \n",
    "COO 형식이 위치 배열값을 중복적으로 가지는 문제 해결방식    \n",
    "희소행렬을 COO, CSR 변환 위해 Scipy의 coo_matrix(), csr_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dense = np.array( [ [ 3, 0, 1 ], \n",
    "                    [0, 2, 0 ] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0 이 아닌 데이터 추출\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성 \n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용하여 COO 형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos,col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "  (0, 0)\t3\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t2\n",
      "<class 'numpy.ndarray'> \n",
      " [[3 0 1]\n",
      " [0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "print(type(sparse_coo))\n",
    "print(sparse_coo)\n",
    "dense01=sparse_coo.toarray()\n",
    "print(type(dense01),\"\\n\", dense01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 희소 행렬 – CSR 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "# 0 이 아닌 데이터 추출\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성 \n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "# COO 형식으로 변환 \n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos,col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "\n",
    "# CSR 형식으로 변환 \n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray())\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "print(sparse_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense3 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
